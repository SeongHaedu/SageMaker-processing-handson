{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c52ebc6-3fd4-4d8a-964f-8133f524831e",
   "metadata": {},
   "source": [
    "# SageMaker Processing Job hands-on üçõ\n",
    "\n",
    "Scope of hands-on\n",
    "\n",
    "* How to prepare data for a SageMaker Processing Job\n",
    "* How to run a SageMaker Processing Job\n",
    "* How to create a Docker image for preprocessing\n",
    "\n",
    "After studying SageMaker Processing Job, you will probably crave some extra spicy curry rice!!! üå∂Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d331e-2f7e-45be-a08a-707618292545",
   "metadata": {},
   "source": [
    "# A whole image of SageMaker Processing Job\n",
    "‚Äª üå∂Ô∏è means a level of this hands-on!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8c0fe-189b-4080-8172-fb9977f5e4cf",
   "metadata": {},
   "source": [
    "## Step. 1: Prepare data to S3 üå∂Ô∏è\n",
    "![overview-preparing](./images/overview-preparing.png)\n",
    "\n",
    "***By user***\n",
    "1. Download data.\n",
    "2. Compress data as Zip format.\n",
    "3. Upload a Zip file to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce720345-74ee-40d7-bf6c-f06aff113acd",
   "metadata": {},
   "source": [
    "## Step. 2: Run a SageMaker Processing Job üå∂Ô∏èüå∂Ô∏èüå∂Ô∏è\n",
    "![overview-processing](./images/overview-processing.png)\n",
    "\n",
    "***By user***\n",
    "1. Build a image for preprocessing.\n",
    "2. Push the image.\n",
    "3. Execute APIs for preprocessing.\n",
    "\n",
    "***By SageMaker***\n",
    "\n",
    "4. Pull the image.\n",
    "5. Download a Zip file.\n",
    "6. Run a processing instance.\n",
    "7. Upload preprocessed data.\n",
    "8. Delete the processing instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5ddc6-a0a1-448e-b6ad-6c00b1f8b71f",
   "metadata": {},
   "source": [
    "# SageMaker Processing Job hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a93fbcb-d7dd-41b1-9bae-29d529f0a8f0",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Run a SageMaker Processing job using the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31820a7a-7422-4765-aafd-9308dccaf8e3",
   "metadata": {},
   "source": [
    "## Step. 1: Prepare data to S3 üå∂Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d6a863-df4c-40a2-8c99-292069f5d59a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "print(f'Current sagemaker Version ={sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d338e24-3dcf-49d9-8f01-43799a87189e",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3bef08-8a85-4aef-801a-4592beaaf345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'haedu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8652e19-2a19-4e7e-9bb7-3f8b8ce693b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request, gzip, numpy as np, sagemaker, datetime, yaml, os, shutil\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "key_file = {\n",
    "    'train_img':'train-images-idx3-ubyte.gz',\n",
    "    'train_label':'train-labels-idx1-ubyte.gz',\n",
    "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "# To uniquely identify various identifiers, timestamp is utilized.\n",
    "timestamp = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9))).strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "print(f'timestamp: {timestamp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7edaa1-ec4f-49e7-a5dc-38eb401d44bb",
   "metadata": {},
   "source": [
    "### Download a MNIST data to this notebook instance from internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399547ca-420d-449a-9ea3-a9753c0feae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# a directory for saving image files and labels.\n",
    "dataset_dir = './mnist/'\n",
    "\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "for v in key_file.values():\n",
    "    file_path = dataset_dir + '/' + v\n",
    "    # download data\n",
    "    urllib.request.urlretrieve(url_base + v, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c00b02-5b9a-4fe2-9982-6f891ed881ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls  {dataset_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f997ad-879b-4a11-89fc-4aaaa02abeed",
   "metadata": {},
   "source": [
    "### Convert MNIST binary data into NumPy arrays.\n",
    "Encode the label data using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4862a41c-ed97-4255-b0e0-7878e6ac9b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "file_path = dataset_dir + key_file['train_img']\n",
    "with gzip.open(file_path, 'rb') as f:\n",
    "    train_x = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28,28)\n",
    "file_path = dataset_dir + key_file['train_label']\n",
    "with gzip.open(file_path, 'rb') as f:\n",
    "    train_y = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "train_y = np.identity(10)[train_y]\n",
    "\n",
    "file_path = dataset_dir + key_file['test_img']\n",
    "with gzip.open(file_path, 'rb') as f:\n",
    "    test_x = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28,28)\n",
    "file_path = dataset_dir + key_file['test_label']\n",
    "with gzip.open(file_path, 'rb') as f:\n",
    "    test_y = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "test_y = np.identity(10)[test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bdc42-bae0-4fac-974e-454ed25f7190",
   "metadata": {},
   "source": [
    "### Save numpy arrays as PNG format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad04d1-752f-4353-be75-dd563ea99bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# save images in local\n",
    "base_dir = './dataset/'\n",
    "train_x_dir = base_dir + 'train_x/'\n",
    "test_x_dir = base_dir + 'test_x/'\n",
    "\n",
    "os.makedirs(train_x_dir, exist_ok=True)\n",
    "os.makedirs(test_x_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(train_x.shape[0])):\n",
    "    Image.fromarray(train_x[i,:,:]).save(train_x_dir + str(i).zfill(5) + \".png\")\n",
    "\n",
    "for i in tqdm(range(test_x.shape[0])):\n",
    "    Image.fromarray(test_x[i,:,:]).save(test_x_dir + str(i).zfill(5) + \".png\")\n",
    "\n",
    "np.save(base_dir + 'train_y.npy',train_y)\n",
    "np.save(base_dir + 'test_y.npy',test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75487e-dec7-4cf7-b3b2-db7fc267a111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls {base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd5408-15b4-4a33-bc41-90ffffa03cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls {base_dir}train_x | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412043e-3a11-4f57-b67a-28fc31c96427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls {base_dir}test_x | head -n5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af5e997-ea74-4fca-8e4e-1311b241d5f1",
   "metadata": {},
   "source": [
    "### Compress the mnist data as a Zip file\n",
    "\n",
    "* When performing preprocessing, a zip file improves transfer efficiency from S3 to the container (transferring smaller individual files takes more time). \n",
    "* The decompression process should also be included in the preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc7b2d-357f-4245-b11b-feca2e6d605b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zip_file = shutil.make_archive('./dataset', 'zip', root_dir='./dataset/')\n",
    "print(zip_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f55c82-a544-41c9-8d51-17d990b3169f",
   "metadata": {},
   "source": [
    "### Upload the Zip file using the SageMaker SDK to S3\n",
    "\n",
    "By using the upload_data method, you can upload data to the SageMaker default bucket (sagemaker-{region}-{account}) with just one line of code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e889f-cd88-4f90-a923-c46130fd5321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = f'sagemaker-handson-{name}/dataset-{timestamp}'\n",
    "zip_dataset_s3_uri = sagemaker.session.Session().upload_data(path=zip_file, key_prefix=prefix)\n",
    "\n",
    "print(zip_dataset_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657a0d1-4985-4830-be58-4e6e2318b0ff",
   "metadata": {},
   "source": [
    "## Step. 2: Run a SageMaker Processing Job üå∂Ô∏èüå∂Ô∏èüå∂Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fad1c-31ad-46d0-9941-aa7a698adc90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.s3 import parse_s3_url\n",
    "import yaml,boto3, io\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "print(f'role: {role}')\n",
    "print(f'name: {name}')\n",
    "print(f'zip_dataset_s3_uri: {zip_dataset_s3_uri}')\n",
    "print(f'timestamp: {timestamp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51ff27-376e-48b1-bc11-92e7af7d4f2b",
   "metadata": {},
   "source": [
    "### Create a container image for preprocessing\n",
    "SageMaker provides built-in containers for Apache Spark and scikit-learn, but there are no libraries for image processing (scikit-image, opencv, pillow, etc.) in default containers. \n",
    "But, you can use these libraries using a option of bring your own container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95293ddf-a6e0-4692-94e1-9c8ecb28c423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat ./container/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015d1b1-3319-4909-8abc-4056770f2dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a image\n",
    "%cd container\n",
    "!docker build -t sagemaker-tf-handson-{name}-{timestamp} .\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea92693-d7c4-47d1-b34f-d2623cd647ba",
   "metadata": {},
   "source": [
    "SageMaker Notebooks pre-installed Docker. (In SageMaker Studio, the Docker is not installed. So, you have to use sm-docker commands instead of docker commands)\n",
    "1. Build the image locally.\n",
    "2. Push the image to an Elastic Container Registry repository.\n",
    "3. Use the pushed image to perform preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3359e61-b1f3-4235-9a7d-59fdac389327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Retrieve a AWS account info for ECR\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "tag = ':latest'\n",
    "\n",
    "ecr_repository = f'sagemaker-tf-handson-{name}-{timestamp}'\n",
    "image_uri = f'{account_id}.dkr.ecr.{region}.amazonaws.com/{ecr_repository+tag}'\n",
    "\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    " \n",
    "# Create a repository\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    " \n",
    "!docker build -t {ecr_repository} .\n",
    "!docker tag {ecr_repository + tag} $image_uri\n",
    "!docker push $image_uri\n",
    "\n",
    "print(f'This container image was registered in {image_uri}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187d498-f879-483d-a6c6-cd3b0f72baca",
   "metadata": {},
   "source": [
    "### Directories of data to run a processing instance\n",
    "Create a processing instance to perform preprocessing and start the job. Set the directory where the processor will perform the processing and the job name in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca2b1b-803f-4eea-9ab6-3ecb8d322a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processing_input_dir = '/opt/ml/processing/input'\n",
    "processing_output_train_dir = '/opt/ml/processing/train'\n",
    "processing_output_test_dir = '/opt/ml/processing/test'\n",
    "job_name = f'sagemaker-preprocess-handson-{name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741925a-da2f-4388-95d2-c07dba6a9a9a",
   "metadata": {},
   "source": [
    "### Run a processing instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99aeb3-b334-4609-83fa-9de2c4077ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize ./preprocessing_script/preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89799e0e-9343-440e-bee5-da3e20d5929d",
   "metadata": {},
   "source": [
    "* To process with your own Docker image, create a processing instance from a ScriptProcessor class. \n",
    "* Specify the URI of the image you pushed earlier in `image_uri`. \n",
    "* Additionally, since the image you created has the Python 3.7 path set up for `python3`, specify `python3` in the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ddfac2-a7ca-4d04-91c4-b75a7885b1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor = ScriptProcessor(\n",
    "    base_job_name=job_name,\n",
    "    image_uri=image_uri,\n",
    "    command=['python3'],\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05983d5c-678f-40b6-9921-5a58e46b6826",
   "metadata": {},
   "source": [
    "* After creating a processor instance, you can run a SageMaker Processing Job using the `run` method. \n",
    "* Specify a `.py` file in the code argument. \n",
    "* Pass values to the processing script through the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904044e3-d20e-4fba-b1db-3da68e753873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run a Processing Job to decompress a zip file and execute a method of histogram equalization\n",
    "processor.run(\n",
    "    code='./preprocessing_script/preprocess.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(source=zip_dataset_s3_uri,destination=processing_input_dir)\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name='train',source=processing_output_train_dir),\n",
    "        ProcessingOutput(output_name='test',source=processing_output_test_dir)],\n",
    "    arguments=[\n",
    "        '--hist-flatten', 'True',\n",
    "        '--input-dir',processing_input_dir,\n",
    "        '--output-train-dir',processing_output_train_dir,\n",
    "        '--output-test-dir',processing_output_test_dir\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3703d15f-1eba-4d6d-a8f2-e0ab3d8ece08",
   "metadata": {},
   "source": [
    "By using the describe method, you can understand the execution results of a job (such as the output destination for data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ae53d-c410-41d4-b24d-0564301ff683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# describe the detail info of the preprocessing job run \n",
    "processor_description = processor.jobs[-1].describe()\n",
    "print(processor_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a7679c-abfa-443e-9cc8-e8cd720dc955",
   "metadata": {},
   "source": [
    "### Confirm the execution result\n",
    "* The execution results are stored in S3. \n",
    "* This time, you can confirm that npy files are loaded with numpy, and an example of the pre-processed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b44229-c85a-47b3-9039-1f2eeb4a0946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_uri = processor_description['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\n",
    "test_data_uri = processor_description['ProcessingOutputConfig']['Outputs'][1]['S3Output']['S3Uri']\n",
    "print(f'train_data_uri: {train_data_uri}')\n",
    "print(f'test_data_uri: {test_data_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb3cf0-e0af-4864-a99d-cc008d4fbb87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket,train_key = parse_s3_url(train_data_uri)\n",
    "bucket,test_key = parse_s3_url(test_data_uri)\n",
    "s3 = boto3.client('s3')\n",
    "obj_list=s3.list_objects_v2(Bucket=bucket, Prefix=train_key)\n",
    "file=[]\n",
    "for contents in obj_list['Contents']:\n",
    "    file.append(contents['Key'])\n",
    "obj_list=s3.list_objects_v2(Bucket=bucket, Prefix=test_key)\n",
    "for contents in obj_list['Contents']:\n",
    "    file.append(contents['Key'])\n",
    "\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1fa1d3-dda5-44b8-8dd0-d42721855239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = boto3.client('s3').get_object(Bucket = bucket, Key = file[0])[\"Body\"].read()\n",
    "train_x = np.load(io.BytesIO(res))\n",
    "plt.imshow(train_x[100,:,:,0],'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeecc9c-2486-4e65-b982-3aea6f4d8591",
   "metadata": {},
   "source": [
    "## Let's go to eat an extra spicy curry rice!!! üçõüå∂Ô∏è\n",
    "![extra-spicy-curry](./images/extra-spicy-curry.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
